{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wkxwell/Team63_DLH_SPRING2024/blob/main/Team63_DLH_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ],
      "metadata": {
        "id": "j01aH0PR4Sg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Team 63 - Kaixin Wang, XingChen Wu\n",
        "Kaixin5@illinois.edu Kaixin5\n",
        "Xw82@illinois.edu xw82\n",
        "\n",
        "\n",
        "Our team referenced research paper \"A machine learning approach to identifying delirium from electronic health records\" to analyze the detection of delirium.\n",
        "\n",
        "The identication of it has always been difficult due to inadepquate assessment and under-documentation. Many of the cases are identified after a period of meidication usage or ICU admission. The focus of our chosen paper is to present a classification model that identifies delirium using retrospective EHR data. The goal of our team is to understand the problem and method introduced by the paper in order to replicate it to achieve similar results based on MIMIC III health datasets. We want to further prove the point that through logistic regression model and multi-layer perception can demosntrate a high accuracy with a mean AU of 0.87.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code comment is used as inline annotations for your coding\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks\""
      ],
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ri2Vo7oEPBqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "The main hypothesis we are going to test will align with the research paper:\n",
        "\n",
        "  Machine learning models can identify delirium from electronic health record data with greater accuracy than traditional screening methods.\n",
        "\n",
        "The project duration is estimated to be just a little over a month, the initial phase will focus on the data retrieve and processing to ensure we have access to all neccessary criteria/weights needed for training conduct. Due to the overwhelm size of the orignial dataset and time/resource limitations, we are using a subset of the dataset to perform the logistic model training."
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "LM4WUjz64C3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
        "'''\n",
        "# mount this notebook to your google drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# define dirs to workspace and data\n",
        "img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n",
        "\n",
        "import cv2\n",
        "img = cv2.imread(img_dir)\n",
        "cv2.imshow(\"Title\", img)\n"
      ],
      "metadata": {
        "id": "rRksCB1vbYwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "The data we used in this research project is coming from the MIMIC III Health datasets which aligns with the research paper. The research paper used this dataset as a cross validation. However due to certain data being unavailble as the original dataset were from NYU CUIMC which we are unable to retrieve. Certain modifications are done to the dataset to make the usable in our context."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XzVUQS0CHry0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# dir and function to load raw data\n",
        "raw_data_dir = '/content/drive/MyDrive/Colab Notebooks/Data'\n",
        "\n",
        "# The data is from MIMIC-III databases, I select 65 patients, tables I used are\n",
        "# CHARTEVENTS and PATIENTS, I joined the two tables using other softwares.\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "    df = pd.read_csv(raw_data_dir+'/master.csv')\n",
        "    return df\n",
        "\n",
        "raw_data = load_raw_data(raw_data_dir)\n",
        "print(raw_data.head())\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(raw_data):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "  gender_counts = raw_data.groupby('gender')['subject_id'].nunique()\n",
        "  itemid_stats = raw_data.groupby('subject_id').size()\n",
        "  item_stat = raw_data.groupby('itemid').size()\n",
        "  avg = itemid_stats.mean()\n",
        "  max = itemid_stats.max()\n",
        "  min = itemid_stats.min()\n",
        "  return gender_counts,avg,max,min,item_stat\n",
        "print(calculate_stats(raw_data))\n",
        "\n",
        "\n",
        "# Data is preprocessed through other softwares into  three files\n",
        "# demo_records.pkl: The first two value denotes male or female.\n",
        "# [1,0] for male and [0,1] for female or vice versa.\n",
        "# Age and Elixhauser index were normalized. Ex: [1, 0, 0.5, 0.4]\n",
        "# patient_records.pkl: Drug exposure and diagnoses were one-hot encoded.\n",
        "# Ex: [1, 0, 0, ..., 0, 1]\n",
        "# labels.pkl: 1 indicates delirium\n",
        "\n",
        "\n",
        "# process raw data\n",
        "#def process_data(raw_data):\n",
        "    # implement this function to process the data as you need\n",
        "  #return None\n",
        "\n",
        "#processed_data = process_data(raw_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "znnLQQW2tbaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class my_model():\n",
        "  # use this class to define your model\n",
        "  pass\n",
        "\n",
        "\n",
        "#This is the implementation of the logistic regression model using TensorFlow.\n",
        "#Consist of training the model with cross-validation, handling data loading, preprocessing, and saving model metrics.\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import os\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "#Extends Tensorflows\n",
        "#Includes layers of concatenation of features and a dense layer of logistic regression wiht L2 regularization\n",
        "class LogisticRegression(tf.keras.Model):\n",
        "    def __init__(self, config):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.optimizer = tf.keras.optimizers.Adam(config[\"learning_rate\"])\n",
        "\n",
        "        self.concatenation = tf.keras.layers.Concatenate(axis=1, name=\"concatenation\")\n",
        "        self.lr = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid, name=\"lr\",\n",
        "        kernel_regularizer=tf.keras.regularizers.L2(l2=config[\"l2_reg\"]))\n",
        "\n",
        "    def call(self, x, d):\n",
        "        x = unit_normalization(x)\n",
        "        return self.lr(self.concatenation([x, d]))\n",
        "\n",
        "#computes a negative log likelihood for a binary classifcation.\n",
        "def compute_loss(model, x, d, label):\n",
        "    prediction = model(x, d)\n",
        "    loss_sum = tf.negative(tf.add(tf.multiply(5, tf.multiply(label, tf.math.log(prediction))),\n",
        "                                  tf.multiply(tf.subtract(1., label), tf.math.log(tf.subtract(1., prediction)))))\n",
        "    return tf.reduce_mean(loss_sum)\n",
        "\n",
        "def calculate_auc(model, test_x, test_d, test_y, config):\n",
        "    AUC = tf.keras.metrics.AUC(num_thresholds=200)\n",
        "    AUC.reset_states()\n",
        "    x, d, y = pad_matrix(test_x, test_d, test_y, config)\n",
        "    pred = model(x, d)\n",
        "    #print(pred)\n",
        "    AUC.update_state(y, pred)\n",
        "\n",
        "    return AUC.result().numpy()\n",
        "\n",
        "def calculate_ROC(model, test_x, test_d, test_y, config):\n",
        "    x, d, y = pad_matrix(test_x, test_d, test_y, config)\n",
        "    pred = model(x,d)\n",
        "    fpr, tpr, thresholds = roc_curve(test_y, pred)\n",
        "    return fpr, tpr, thresholds\n",
        "\n",
        "def calculate_ppv(model, test_x, test_d, test_y, config):\n",
        "    ppv = tf.keras.metrics.Precision(thresholds=0.8)\n",
        "    ppv.reset_states()\n",
        "    x, d, y = pad_matrix(test_x, test_d, test_y, config)\n",
        "    pred = model(x,d)\n",
        "    ppv.update_state(y, pred)\n",
        "    return ppv.result().numpy()\n",
        "\n",
        "#to load, save shuffle and pad data\n",
        "def load_data(patient_record_path, demo_record_path, labels_path):\n",
        "    patient_record = pickle.load(open(patient_record_path, 'rb'))\n",
        "    demo_record = pickle.load(open(demo_record_path, 'rb'))\n",
        "    labels = pickle.load(open(labels_path, 'rb'))\n",
        "\n",
        "    return patient_record, demo_record, labels\n",
        "\n",
        "def save_data(output_path, mydata):\n",
        "    with open(output_path, 'wb') as f:\n",
        "        pickle.dump(mydata, f)\n",
        "\n",
        "def pad_matrix(records, demos, labels, config):\n",
        "    n_patients = len(records)\n",
        "    #input_vocabsize = config[\"input_vocabsize\"]\n",
        "    #demo_vocabsize = config[\"demo_vocabsize\"]\n",
        "\n",
        "    x = np.array(records).astype(np.float32) # sum of all visits of the patient\n",
        "    d = np.array(demos).astype(np.float32)\n",
        "    y = np.array(labels).astype(np.float32)\n",
        "\n",
        "    #for idx, rec in enumerate(records):\n",
        "        #for visit in rec:\n",
        "            #x[idx, visit] += 1\n",
        "\n",
        "    #x = np.clip(0, 1, x) # clip values bigger than 1.\n",
        "\n",
        "    #for idx, demo in enumerate(demos):\n",
        "        #d[idx, int(demo[:-2])] = 1. # the last element of demos is age\n",
        "        #d[idx, -1:] = demo[-1:]\n",
        "\n",
        "    return x, d, y\n",
        "\n",
        "def shuffle_data(data1, data2, data3):\n",
        "    data1, data2, data3 = np.array(data1), np.array(data2), np.array(data3)\n",
        "    idx = np.arange(len(data1))\n",
        "    random.seed(1234)\n",
        "    random.shuffle(idx)\n",
        "\n",
        "    return data1[idx], data2[idx], data3[idx]\n",
        "\n",
        "def unit_normalization(myarray):\n",
        "    avg = tf.reshape(tf.math.reduce_mean(myarray, axis=-1), shape=(myarray.shape[0], 1))\n",
        "    std = tf.reshape(tf.math.reduce_std(myarray, axis=-1), shape=(myarray.shape[0], 1))\n",
        "    return tf.math.divide(tf.math.subtract(myarray, avg), std)\n",
        "def train_lreg_kfold(output_path, patient_record_path, demo_record_path, labels_path, epochs, batch_size,\n",
        "                input_vocabsize, demo_vocabsize, l2_reg=0.001, learning_rate=0.001, k=5, times =5, notes=None):\n",
        "\n",
        "    tf.random.set_seed(1234)\n",
        "    config = locals().copy()\n",
        "    print(config[\"input_vocabsize\"])\n",
        "    for i in range(times):\n",
        "        version = i\n",
        "        print(\"load data...\")\n",
        "        recs, demos, labels = load_data(patient_record_path, demo_record_path, labels_path)\n",
        "\n",
        "        print(\"split the dataset into k-fold...\")\n",
        "        recs, demos, labels = shuffle_data(recs, demos, labels)\n",
        "        chunk_size = int(np.floor(len(labels) / k))\n",
        "        np.split(np.arange(len(labels)), [chunk_size*i for i in range(k)])\n",
        "        folds = np.tile(np.split(np.arange(len(labels)), [chunk_size*i for i in range(int(k))])[1:], 2)\n",
        "        print(len(folds))\n",
        "\n",
        "\n",
        "        k_fold_auc = []\n",
        "        k_fold_ppv = []\n",
        "        k_fold_tpr = []\n",
        "        mean_fpr = np.linspace(0,1,200)\n",
        "        k_fold_training_loss = []\n",
        "\n",
        "        for i in range(k):\n",
        "            train_x, test_x = recs[np.concatenate([folds[j] for j in range(k) if j != i % k])], recs[folds[(i%k)]]\n",
        "            train_d, test_d = demos[np.concatenate([folds[j] for j in range(k) if j != i % k])], demos[folds[(i%k)]]\n",
        "            train_y, test_y = labels[np.concatenate([folds[j] for j in range(k) if j != i % k])], labels[folds[(i%k)]]\n",
        "            print(len(train_y))\n",
        "            print(len(test_y))\n",
        "\n",
        "            num_batches = int(np.ceil(float(len(train_x)) / float(batch_size)))\n",
        "            training_loss = []\n",
        "\n",
        "            print(\"build and initialize model for {k}th fold...\".format(k=i+1))\n",
        "            lr_model = LogisticRegression(config)\n",
        "            #_ = lr_model(train_x, train_d)\n",
        "            #print(lr_model)\n",
        "\n",
        "            best_auc = 0\n",
        "            best_epoch = 0\n",
        "            best_model =  None\n",
        "            #print(best_model)\n",
        "            print(\"start training...\")\n",
        "            for epoch in range(epochs):\n",
        "                loss_record = []\n",
        "                progbar = tf.keras.utils.Progbar(num_batches)\n",
        "\n",
        "                for t in random.sample(range(num_batches), num_batches):\n",
        "                    batch_x = train_x[t * batch_size:(t+1) * batch_size]\n",
        "                    batch_d = train_d[t * batch_size:(t+1) * batch_size]\n",
        "                    batch_y = train_y[t * batch_size:(t+1) * batch_size]\n",
        "\n",
        "                    x, d, y = pad_matrix(batch_x, batch_d, batch_y, config)\n",
        "\n",
        "                    with tf.GradientTape() as tape:\n",
        "                        batch_cost = compute_loss(lr_model, x, d, y)\n",
        "                    gradients = tape.gradient(batch_cost, lr_model.trainable_variables)\n",
        "                    lr_model.optimizer.apply_gradients(zip(gradients, lr_model.trainable_variables))\n",
        "\n",
        "                    loss_record.append(batch_cost.numpy())\n",
        "                    progbar.add(1)\n",
        "\n",
        "                print('epoch:{e}, mean loss:{l:.6f}'.format(e=epoch+1, l=np.mean(loss_record)))\n",
        "                training_loss.append(np.mean(loss_record))\n",
        "                current_auc = calculate_auc(lr_model, test_x, test_d, test_y, config)\n",
        "                print(current_auc)\n",
        "                #print(lr_model.get_weights())\n",
        "                #print('epoch:{e}, current auc:{l:.6f}'.format(e=epoch+1, l=current_auc))\n",
        "                if current_auc > best_auc:\n",
        "                    best_auc = current_auc\n",
        "                    best_epoch = epoch+1\n",
        "                    best_model = lr_model.get_weights()\n",
        "\n",
        "            k_fold_training_loss.append(training_loss)\n",
        "            print(\"calculate AUC on the best model using the test set\")\n",
        "            lr_model.set_weights(best_model)\n",
        "            test_auc = calculate_auc(lr_model, test_x, test_d, test_y, config)\n",
        "            test_ppv = calculate_ppv(lr_model, test_x, test_d, test_y, config)\n",
        "            print(\"AUC of {k}th fold: {auc:.6f}\".format(k=i+1, auc=test_auc))\n",
        "            print(\"PPV of {k}th fold: {ppv:.6f}\".format(k=i+1, ppv=test_ppv))\n",
        "            k_fold_auc.append(test_auc)\n",
        "            k_fold_ppv.append(test_ppv)\n",
        "            fpr, tpr, thresholds = calculate_ROC(lr_model, test_x, test_d, test_y, config)\n",
        "            k_fold_tpr.append(np.interp(mean_fpr, fpr, tpr))\n",
        "\n",
        "        print(\"saving k-fold results...\")\n",
        "        mode_name = \"mhot\"\n",
        "        #np.save(os.path.join(output_path, \"LRS_{m}_{k}fold_l{l}_training_loss_ver{i}.npy\".format(k=k, m=mode_name, l=learning_rate, i=version)), k_fold_training_loss)\n",
        "        np.save(os.path.join(output_path, \"LRS_{m}_{k}fold_l{l}_auc_ver{i}.npy\".format(k=k, m=mode_name, l=learning_rate, i=version)), k_fold_auc)\n",
        "        np.save(os.path.join(output_path, \"LRS_{m}_{k}fold_l{l}_tpr_ver{i}.npy\".format(k=k, m=mode_name, l=learning_rate, i=version)), k_fold_tpr)\n",
        "        np.save(os.path.join(output_path, \"LRS_{m}_{k}fold_l{l}_ppv_ver{i}.npy\".format(k=k, m=mode_name, l=learning_rate, i=version)), k_fold_ppv)\n",
        "        #np.save(os.path.join(output_path, \"LRS_{m}_e{e}_l{l}_model_ver{i}.npy\".format(m=mode_name, e=epochs, l=learning_rate, i=version)), lr_model.get_weights())\n",
        "\n",
        "    save_data(os.path.join(output_path, \"LRS_{m}_{k}fold_l{l}_config.pkl\".format(k=k, m=mode_name, l=learning_rate)), config)\n",
        "\n",
        "train_lreg_kfold('/content/drive/MyDrive/Colab Notebooks/Data/ml_output', '/content/drive/MyDrive/Colab Notebooks/Data/patient_records.pkl','/content/drive/MyDrive/Colab Notebooks/Data/demo_records.pkl','/content/drive/MyDrive/Colab Notebooks/Data/labels.pkl', 20, 2,100, 4, l2_reg=0.001, learning_rate=0.001, k=5, times=5)\n"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "In this draft version of the project implementation we noticed from the graph below we are getting a score of 0.708 AUC instead of the 0.87 AUC achieved in the research paper. We analyzed this difference is due to the dataset limitation as we do not have access to the NYU UCIMC, we will increase the sub-dataset from the MIMIC III in the next two weeks and continuing to tweak the parameters to achieve higher AUC.\n",
        "\n",
        "But overall according to this 0.708 AUC it sugguests that the training model discussed in the paper has good discriminative ability. It means that there is a 78% chance that the model will be able to distinguish between the positive class and negative class correctly.\n",
        "\n",
        "Therefore in this draft version of the project, our goal is to continue tweak the parameters and increase sub-dataset size to achieve higher AUC as they did in the research paper.\n",
        "\n",
        "So far this model has a reasonbly strong ability to classify the positive and negative cases correctly across different thresholds. It is a valuable metric when deciding whether the model's predictive accuracy is sufficient for pratical applications."
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation.\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "tpr_LRS = np.load('/content/drive/MyDrive/Colab Notebooks/Data/ml_output/LRS_mhot_5fold_l0.001_tpr_ver4.npy')\n",
        "auc_LRS = np.load('/content/drive/MyDrive/Colab Notebooks/Data/ml_output/LRS_mhot_5fold_l0.001_auc_ver4.npy')\n",
        "mean_tpr_LRS = np.mean(tpr_LRS, axis=0)\n",
        "mean_auc_LRS = np.mean(auc_LRS, axis=0)\n",
        "mean_fpr = np.linspace(0,1,200)\n",
        "f = plt.figure(figsize=(6, 4))\n",
        "\n",
        "plt.figure(1)\n",
        "plt.xlim(0,1)\n",
        "plt.ylim(0,1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(mean_fpr, mean_tpr_LRS, color='blue',label=r'LR (AUC = %0.3f)' % (mean_auc_LRS),lw=2, alpha=1)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "f.savefig(\"./ROC_all.pdf\", bbox_inches='tight')"
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Sun, J, [paper title], [journal title], [year], [volume]:[issue], doi: [doi link to paper]\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feel free to add new sections"
      ],
      "metadata": {
        "id": "xmVuzQ724HbO"
      }
    }
  ]
}